{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7fb597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_file(file_path, real_data_path, output_path):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from scipy.stats import ks_2samp, wasserstein_distance\n",
    "    from scipy.spatial.distance import jensenshannon\n",
    "    from sklearn.metrics.pairwise import rbf_kernel\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "\n",
    "    def approximate_diversity(X, n_samples=1000000, random_state=42):\n",
    "        np.random.seed(random_state)\n",
    "        n = X.shape[0]\n",
    "        idx_1 = np.random.randint(0, n, size=n_samples)\n",
    "        idx_2 = np.random.randint(0, n, size=n_samples)\n",
    "        mask = idx_1 != idx_2\n",
    "        return np.mean(np.linalg.norm(X[idx_1[mask]] - X[idx_2[mask]], axis=1))\n",
    "\n",
    "    def vendi_score(X, gamma=1e-5, normalize=True):\n",
    "        K = rbf_kernel(X, gamma=gamma)\n",
    "        if normalize:\n",
    "            trace = np.trace(K)\n",
    "            if trace == 0:\n",
    "                raise ValueError(\"Trace nulle.\")\n",
    "            K /= trace\n",
    "        eigvals = np.linalg.eigvalsh(K)\n",
    "        eigvals = np.clip(eigvals, 1e-12, 1.0)\n",
    "        entropy = -np.sum(eigvals * np.log(eigvals))\n",
    "        return np.exp(entropy)\n",
    "\n",
    "    # nom de base sans extension\n",
    "    nom = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    print(f\"\\nüü¢ Traitement de : {nom}\")\n",
    "\n",
    "    data = pd.read_csv(real_data_path)\n",
    "    synthetic_data = pd.read_csv(file_path)\n",
    "    real_data = data.copy()\n",
    "\n",
    "    for col in real_data.columns:\n",
    "        if real_data[col].dtype == bool:\n",
    "            real_data[col] = real_data[col].astype(float)\n",
    "            synthetic_data[col] = synthetic_data[col].astype(float)\n",
    "\n",
    "    # 1. Tests de distributions\n",
    "    print(\"‚ñ∂Ô∏è  [1] KS / Wasserstein / JSD\")\n",
    "    ks_results = []\n",
    "    equal_count, different_count = 0, 0\n",
    "    for column in real_data.columns:\n",
    "        if column == 'target':\n",
    "            continue\n",
    "        real_values = real_data[column].values\n",
    "        synthetic_values = synthetic_data[column].values\n",
    "        result = {'Feature': column}\n",
    "        ks_statistic, ks_p_value = ks_2samp(real_values, synthetic_values)\n",
    "        result['KS Statistic'] = ks_statistic\n",
    "        result['KS P-value'] = ks_p_value\n",
    "        equal_count += int(ks_p_value > 0.05)\n",
    "        different_count += int(ks_p_value <= 0.05)\n",
    "        result['Wasserstein Distance'] = wasserstein_distance(real_values, synthetic_values)\n",
    "        hist_range = (min(real_values.min(), synthetic_values.min()), max(real_values.max(), synthetic_values.max()))\n",
    "        p_hist, _ = np.histogram(real_values, bins=100, range=hist_range, density=True)\n",
    "        q_hist, _ = np.histogram(synthetic_values, bins=100, range=hist_range, density=True)\n",
    "        p_hist += 1e-8\n",
    "        q_hist += 1e-8\n",
    "        p_hist /= p_hist.sum()\n",
    "        q_hist /= q_hist.sum()\n",
    "        result['Jensen-Shannon Divergence'] = jensenshannon(p_hist, q_hist, base=2) ** 2\n",
    "        ks_results.append(result)\n",
    "\n",
    "    ks_results_df = pd.DataFrame(ks_results)\n",
    "\n",
    "    # 2. CB Diff\n",
    "    print(\"‚ñ∂Ô∏è  [2] Distribution des classes (CB Diff)\")\n",
    "    real_dist = real_data['target'].value_counts(normalize=True)\n",
    "    synth_dist = synthetic_data['target'].value_counts(normalize=True)\n",
    "    all_classes = sorted(set(real_dist.index) | set(synth_dist.index))\n",
    "    cb_diff = np.sum(np.abs(real_dist.reindex(all_classes, fill_value=0) - synth_dist.reindex(all_classes, fill_value=0))) * 100\n",
    "\n",
    "    # 3.1 TSTR - RandomForest\n",
    "    print(\"‚ñ∂Ô∏è  [3.1] TSTR - RandomForestClassifier\")\n",
    "    X = real_data.drop(columns='target')\n",
    "    y = real_data['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    X_synthetic = synthetic_data.drop(columns='target').reindex(columns=X_test.columns)\n",
    "    y_synthetic = synthetic_data['target']\n",
    "    model_rf = RandomForestClassifier(random_state=42)\n",
    "    model_rf.fit(X_synthetic, y_synthetic)\n",
    "    y_pred_rf = model_rf.predict(X_test)\n",
    "    tstr_rf_acc = accuracy_score(y_test, y_pred_rf)\n",
    "    tstr_rf_f1 = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "\n",
    "    # 3.2 TSTR - MLP avec TensorFlow (avec ModelCheckpoint)\n",
    "    print(\"‚ñ∂Ô∏è  [3.2] TSTR - MLP (Keras)\")\n",
    "    X_synthetic_np = X_synthetic.values\n",
    "    y_synthetic_np = y_synthetic.values\n",
    "    X_test_np = X_test.values\n",
    "    y_test_np = y_test.values\n",
    "\n",
    "    input_dim = X_synthetic_np.shape[1]\n",
    "    num_classes = len(np.unique(y_test_np))\n",
    "    loss_fn = 'sparse_categorical_crossentropy' if num_classes > 2 else 'binary_crossentropy'\n",
    "    activation_out = 'softmax' if num_classes > 2 else 'sigmoid'\n",
    "\n",
    "    model_mlp = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(input_dim,)),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation=activation_out)\n",
    "    ])\n",
    "\n",
    "    model_mlp.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    checkpoint_path = \"./temp_best_mlp.weights.h5\"\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path, monitor='val_accuracy', save_best_only=True, save_weights_only=True, verbose=0\n",
    "    )\n",
    "\n",
    "    model_mlp.fit(\n",
    "        X_synthetic_np, y_synthetic_np,\n",
    "        validation_data=(X_test_np, y_test_np),\n",
    "        epochs=50,\n",
    "        batch_size=128,\n",
    "        callbacks=[checkpoint_cb],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    model_mlp.load_weights(checkpoint_path)\n",
    "\n",
    "    y_pred_probs = model_mlp.predict(X_test_np, verbose=0)\n",
    "    y_pred_mlp = np.argmax(y_pred_probs, axis=1) if num_classes > 2 else (y_pred_probs > 0.5).astype(int).flatten()\n",
    "    tstr_mlp_acc = accuracy_score(y_test_np, y_pred_mlp)\n",
    "    tstr_mlp_f1 = f1_score(y_test_np, y_pred_mlp, average='weighted')\n",
    "\n",
    "    os.remove(checkpoint_path)\n",
    "\n",
    "\n",
    "    # 4. Approximate Diversity (√©cart relatif pond√©r√©)\n",
    "    print(\"‚ñ∂Ô∏è  [4] Approximate Diversity (√©cart relatif pond√©r√©)\")\n",
    "\n",
    "    gap_weighted_total = 0\n",
    "    weight_total = 0\n",
    "    all_classes = sorted(set(real_data['target'].unique()) | set(synthetic_data['target'].unique()))\n",
    "\n",
    "    for cls in all_classes:\n",
    "        real_cls = real_data[real_data['target'] == cls]\n",
    "        synth_cls = synthetic_data[synthetic_data['target'] == cls]\n",
    "\n",
    "        if len(real_cls) < 2 or len(synth_cls) < 2:\n",
    "            continue\n",
    "\n",
    "        X_real_cls = real_cls.drop(columns='target').values\n",
    "        X_synth_cls = synth_cls.drop(columns='target').values\n",
    "\n",
    "        div_real_cls = approximate_diversity(X_real_cls)\n",
    "        div_synth_cls = approximate_diversity(X_synth_cls)\n",
    "        gap_cls = (div_synth_cls - div_real_cls) / div_real_cls * 100\n",
    "\n",
    "        weight = len(real_cls) / len(real_data)\n",
    "        gap_weighted_total += weight * gap_cls\n",
    "        weight_total += weight\n",
    "\n",
    "    div_gap = gap_weighted_total / weight_total\n",
    "\n",
    "\n",
    "    # 5. Vendi Score\n",
    "    print(\"‚ñ∂Ô∏è  [5] Vendi Score\")\n",
    "    n_real = min(3000, len(X))\n",
    "    n_synth = min(3000, len(X_synthetic))\n",
    "    X_real_sub = X.sample(n=n_real, random_state=42).values\n",
    "    X_synth_sub = X_synthetic.sample(n=n_synth, random_state=42).values\n",
    "    vendi_real = vendi_score(X_real_sub)\n",
    "    vendi_synth = vendi_score(X_synth_sub)\n",
    "    vendi_gap = (vendi_synth - vendi_real) / vendi_real * 100\n",
    "\n",
    "    # 6. Moyenne / Variance\n",
    "    print(\"‚ñ∂Ô∏è  [6] Moyenne et variance des features synth√©tiques\")\n",
    "    synth_mean = X_synthetic.mean().mean()\n",
    "    synth_var = X_synthetic.var().mean()\n",
    "\n",
    "    # R√©sum√©\n",
    "    summary = {\n",
    "        'Feature': 'GLOBAL',\n",
    "        'KS Statistic': ks_results_df['KS Statistic'].mean(),\n",
    "        'KS P-value': ks_results_df['KS P-value'].mean(),\n",
    "        'Wasserstein Distance': ks_results_df['Wasserstein Distance'].mean(),\n",
    "        'Jensen-Shannon Divergence': ks_results_df['Jensen-Shannon Divergence'].mean(),\n",
    "        '# Features': len(X.columns),\n",
    "        '# Similaires (p > 0.05)': equal_count,\n",
    "        '# Diff√©rentes': different_count,\n",
    "        'PD (%)': 100 * different_count / len(X.columns),\n",
    "        'CB Diff (%)': cb_diff,\n",
    "        'TSTR RF Accuracy': tstr_rf_acc,\n",
    "        'TSTR RF F1-score': tstr_rf_f1,\n",
    "        'TSTR MLP Accuracy': tstr_mlp_acc,\n",
    "        'TSTR MLP F1-score': tstr_mlp_f1,\n",
    "        'Diversity Gap (%)': div_gap,\n",
    "        'Vendi Score (real)': vendi_real,\n",
    "        'Vendi Score (synth)': vendi_synth,\n",
    "        'Vendi Gap (%)': vendi_gap,\n",
    "        'Mean (synth)': synth_mean,\n",
    "        'Variance (synth)': synth_var\n",
    "    }\n",
    "\n",
    "    ks_results_df = pd.concat([ks_results_df, pd.DataFrame([summary])], ignore_index=True)\n",
    "    output_file = os.path.join(output_path, f\"results_{nom}.csv\")\n",
    "    print(f\"üíæ  Sauvegarde dans : {output_file}\")\n",
    "    ks_results_df.to_csv(output_file, index=False)\n",
    "    print(\"‚úÖ  Termin√©.\\n\")\n",
    "    #Clear memory\n",
    "    del model_mlp, model_rf\n",
    "    from tensorflow.keras import backend as K\n",
    "    import gc\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    return ks_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34901c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:52:20.005311: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-17 13:52:20.412904: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744890740.591819     896 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744890740.636789     896 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744890741.035314     896 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744890741.035488     896 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744890741.035494     896 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744890741.035499     896 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-17 13:52:21.080648: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ Traitement de : cluster_vae_df_eps_0pt5\n",
      "‚ñ∂Ô∏è  [1] KS / Wasserstein / JSD\n",
      "‚ñ∂Ô∏è  [2] Distribution des classes (CB Diff)\n",
      "‚ñ∂Ô∏è  [3.1] TSTR - RandomForestClassifier\n",
      "‚ñ∂Ô∏è  [3.2] TSTR - MLP (Keras)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744891931.440319     896 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1768 MB memory:  -> device: 0, name: NVIDIA RTX A1000 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744891933.928098   43758 service.cc:152] XLA service 0x7f7bb401d610 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1744891933.928134   43758 service.cc:160]   StreamExecutor device (0): NVIDIA RTX A1000 Laptop GPU, Compute Capability 8.6\n",
      "2025-04-17 14:12:13.971244: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1744891934.085878   43758 cuda_dnn.cc:529] Loaded cuDNN version 90701\n",
      "2025-04-17 14:12:14.399392: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_169', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:14.911949: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_169_0', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:14.912765: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_169_0', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:14.945128: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_169', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:15.264868: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_176', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:15.402951: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_176', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:16.222764: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_339', 156 bytes spill stores, 268 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:16.231304: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_176', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:16.795050: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_341', 276 bytes spill stores, 276 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:17.074230: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_341', 156 bytes spill stores, 268 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:17.335799: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_339', 276 bytes spill stores, 276 bytes spill loads\n",
      "\n",
      "I0000 00:00:1744891938.899520   43758 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-04-17 14:12:54.760334: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_169', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:55.008880: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_169_0', 144 bytes spill stores, 144 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:55.250376: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_176', 164 bytes spill stores, 168 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:55.287393: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_169_0', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:55.428578: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_339', 204 bytes spill stores, 204 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:55.448323: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_176', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:55.749820: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_176', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:56.107885: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_339', 472 bytes spill stores, 472 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:56.253865: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_339', 436 bytes spill stores, 436 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:56.590175: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_339', 604 bytes spill stores, 604 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:56.739913: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_339', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:56.770544: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_339', 832 bytes spill stores, 832 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:56.799534: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_341', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:57.096685: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_339', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:57.187456: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_339', 508 bytes spill stores, 508 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:57.345612: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_341', 204 bytes spill stores, 204 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:57.345887: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_341', 832 bytes spill stores, 832 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:57.589908: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_341', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2025-04-17 14:12:57.645131: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_341', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2025-04-17 14:13:10.574886: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37_0', 144 bytes spill stores, 144 bytes spill loads\n",
      "\n",
      "2025-04-17 14:13:11.063248: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-04-17 14:13:11.192215: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37_0', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-04-17 14:13:11.315866: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_44', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-04-17 14:13:11.326487: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_44', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-04-17 14:13:11.509137: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_44', 164 bytes spill stores, 168 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ks_results_df = \u001b[43mprocess_one_file\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./generations_cicids/cluster_vae_df_eps_0pt5.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./dataset/cicids2017_clean_all_labels.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./results_cicids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#ks_results_df = process_one_file(\"./generations_nsl/cluster_vae_df_eps_1pt75.csv\", \"./dataset/kdd_full_clean_5classes.csv\", \"./results_nsl\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mprocess_one_file\u001b[39m\u001b[34m(file_path, real_data_path, output_path)\u001b[39m\n\u001b[32m    116\u001b[39m checkpoint_path = \u001b[33m\"\u001b[39m\u001b[33m./temp_best_mlp.weights.h5\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    117\u001b[39m checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n\u001b[32m    118\u001b[39m     checkpoint_path, monitor=\u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m, save_best_only=\u001b[38;5;28;01mTrue\u001b[39;00m, save_weights_only=\u001b[38;5;28;01mTrue\u001b[39;00m, verbose=\u001b[32m0\u001b[39m\n\u001b[32m    119\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[43mmodel_mlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_synthetic_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_synthetic_np\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_np\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m    128\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m model_mlp.load_weights(checkpoint_path)\n\u001b[32m    132\u001b[39m y_pred_probs = model_mlp.predict(X_test_np, verbose=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/esorics/NSLKDD/venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/esorics/NSLKDD/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/esorics/NSLKDD/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/esorics/NSLKDD/venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/esorics/NSLKDD/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/esorics/NSLKDD/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:854\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    852\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwds):\n\u001b[32m    853\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls the graph function.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m854\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lock\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    855\u001b[39m   bound_args = function_type_utils.canonicalize_function_inputs(\n\u001b[32m    856\u001b[39m       args,\n\u001b[32m    857\u001b[39m       kwds,\n\u001b[32m   (...)\u001b[39m\u001b[32m    860\u001b[39m       \u001b[38;5;28mself\u001b[39m._is_pure,\n\u001b[32m    861\u001b[39m   )\n\u001b[32m    862\u001b[39m   args, kwds = bound_args.args, bound_args.kwargs\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "ks_results_df = process_one_file(\"./generations_cicids/cluster_vae_df_eps_0pt5.csv\", \"./dataset/cicids2017_clean_all_labels.csv\", \"./results_cicids\")\n",
    "#ks_results_df = process_one_file(\"./generations_nsl/cluster_vae_df_eps_1pt75.csv\", \"./dataset/kdd_full_clean_5classes.csv\", \"./results_nsl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e0ac19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ Traitement de : cluster_vae_df_eps_0pt5\n",
      "‚ñ∂Ô∏è  [1] KS / Wasserstein / JSD\n",
      "‚ñ∂Ô∏è  [2] Distribution des classes (CB Diff)\n",
      "‚ñ∂Ô∏è  [3.1] TSTR - RandomForestClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m files = glob(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/*.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m tqdm(files):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[43mprocess_one_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mprocess_one_file\u001b[39m\u001b[34m(file_path, real_data_path, output_path)\u001b[39m\n\u001b[32m     87\u001b[39m y_synthetic = synthetic_data[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     88\u001b[39m model_rf = RandomForestClassifier(random_state=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[43mmodel_rf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_synthetic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_synthetic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m y_pred_rf = model_rf.predict(X_test)\n\u001b[32m     91\u001b[39m tstr_rf_acc = accuracy_score(y_test, y_pred_rf)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/esorics/NSLKDD/venv/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/esorics/NSLKDD/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:487\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    476\u001b[39m trees = [\n\u001b[32m    477\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    479\u001b[39m ]\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    486\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    509\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/esorics/NSLKDD/venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/esorics/NSLKDD/venv/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/esorics/NSLKDD/venv/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/esorics/NSLKDD/venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/esorics/NSLKDD/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:189\u001b[39m, in \u001b[36m_parallel_build_trees\u001b[39m\u001b[34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m class_weight == \u001b[33m\"\u001b[39m\u001b[33mbalanced_subsample\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    187\u001b[39m         curr_sample_weight *= compute_sample_weight(\u001b[33m\"\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m\"\u001b[39m, y, indices=indices)\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    197\u001b[39m     tree._fit(\n\u001b[32m    198\u001b[39m         X,\n\u001b[32m    199\u001b[39m         y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m         missing_values_in_feature_mask=missing_values_in_feature_mask,\n\u001b[32m    203\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/esorics/NSLKDD/venv/lib/python3.12/site-packages/sklearn/tree/_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "real_data_path = \"./dataset/kdd_full_clean_5classes.csv\"\n",
    "input_folder = \"./generations\"\n",
    "output_folder = \"./results\"\n",
    "\n",
    "files = glob(f\"{input_folder}/*.csv\")\n",
    "for file_path in tqdm(files):\n",
    "    process_one_file(file_path, real_data_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d1784dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "def load_global_summary(file_path):\n",
    "    \"\"\"\n",
    "    Load a single result CSV file and extract the 'GLOBAL' summary row.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the results_*.csv file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame or None: A one-row DataFrame with the model's summary metrics,\n",
    "      or None if the file doesn't contain a 'GLOBAL' row.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    global_row = df[df['Feature'] == 'GLOBAL'].copy()\n",
    "\n",
    "    if global_row.empty:\n",
    "        print(f\"‚ùå No 'GLOBAL' row found in {file_path}\")\n",
    "        return None\n",
    "\n",
    "    # Extract model name from filename\n",
    "    model_name = os.path.basename(file_path).replace(\"results_\", \"\").replace(\".csv\", \"\")\n",
    "    global_row = global_row.reset_index(drop=True)\n",
    "    global_row.insert(0, \"Model\", model_name)  # Add model name as first column\n",
    "    return global_row\n",
    "\n",
    "\n",
    "def load_all_global_summaries(results_folder):\n",
    "    \"\"\"\n",
    "    Aggregate all 'GLOBAL' summary rows from results_*.csv files in a directory.\n",
    "\n",
    "    Parameters:\n",
    "    - results_folder (str): Path to folder containing results_*.csv files.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame or None: A merged DataFrame with one row per model,\n",
    "      or None if no valid files found.\n",
    "    \"\"\"\n",
    "    files = sorted(glob(f\"{results_folder}/results_*.csv\"))\n",
    "    all_summaries = []\n",
    "\n",
    "    for f in files:\n",
    "        summary = load_global_summary(f)\n",
    "        if summary is not None:\n",
    "            all_summaries.append(summary)\n",
    "\n",
    "    if not all_summaries:\n",
    "        print(\"‚ö†Ô∏è No global summaries found.\")\n",
    "        return None\n",
    "\n",
    "    # Merge all individual summary rows\n",
    "    summary_df = pd.concat(all_summaries, ignore_index=True)\n",
    "\n",
    "    # Organize columns: 'Model' first, others sorted alphabetically\n",
    "    fixed_cols = ['Model']\n",
    "    metric_cols = sorted([col for col in summary_df.columns if col not in fixed_cols])\n",
    "    summary_df = summary_df[fixed_cols + metric_cols]\n",
    "\n",
    "    # Sort rows by model name\n",
    "    summary_df = summary_df.sort_values(by='Model').reset_index(drop=True)\n",
    "\n",
    "    print(f\"‚úÖ Summaries loaded: {len(summary_df)} models\")\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e7b3c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summaries loaded: 57 models\n",
      "                              Model  # Diff√©rentes  # Features  \\\n",
      "0   cluster_vae_df_e20_k40_eps_0pt5           66.0        78.0   \n",
      "1   cluster_vae_df_e20_k40_eps_1pt0           66.0        78.0   \n",
      "2  cluster_vae_df_e20_k40_eps_1pt25           66.0        78.0   \n",
      "3   cluster_vae_df_e20_k40_eps_1pt5           66.0        78.0   \n",
      "4  cluster_vae_df_e20_k40_eps_1pt75           66.0        78.0   \n",
      "\n",
      "   # Similaires (p > 0.05)  CB Diff (%)  Diversity Gap (%) Feature  \\\n",
      "0                     12.0     0.000191         -51.410853  GLOBAL   \n",
      "1                     12.0     0.000191           8.365395  GLOBAL   \n",
      "2                     12.0     0.000191          39.303534  GLOBAL   \n",
      "3                     12.0     0.000191          76.784478  GLOBAL   \n",
      "4                     12.0     0.000191         120.399052  GLOBAL   \n",
      "\n",
      "   Jensen-Shannon Divergence  KS P-value  KS Statistic  ...     PD (%)  \\\n",
      "0                   0.016333    0.153846      0.146846  ...  84.615385   \n",
      "1                   0.007175    0.143432      0.115400  ...  84.615385   \n",
      "2                   0.006446    0.144578      0.121515  ...  84.615385   \n",
      "3                   0.006654    0.150935      0.129364  ...  84.615385   \n",
      "4                   0.007335    0.152833      0.136994  ...  84.615385   \n",
      "\n",
      "   TSTR MLP Accuracy  TSTR MLP F1-score  TSTR RF Accuracy  TSTR RF F1-score  \\\n",
      "0           0.895678           0.893589          0.982345          0.975456   \n",
      "1           0.917505           0.908781          0.982851          0.975949   \n",
      "2           0.901454           0.890171          0.983096          0.976190   \n",
      "3           0.918296           0.911333          0.983031          0.976126   \n",
      "4           0.913267           0.902519          0.983054          0.976152   \n",
      "\n",
      "   Variance (synth)  Vendi Gap (%)  Vendi Score (real)  Vendi Score (synth)  \\\n",
      "0      5.945949e+13       9.253780         2736.290838          2989.501183   \n",
      "1      9.459878e+13       9.006906         2736.290838          2982.745985   \n",
      "2      1.023971e+14       8.804148         2736.290838          2977.197933   \n",
      "3      1.079962e+14       8.628757         2736.290838          2972.398736   \n",
      "4      1.121083e+14       8.981019         2736.290838          2982.037641   \n",
      "\n",
      "   Wasserstein Distance  \n",
      "0         724284.089147  \n",
      "1         107377.095436  \n",
      "2         202480.938490  \n",
      "3         353648.686964  \n",
      "4         464780.776731  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "üíæ Fichier de synth√®se sauvegard√© dans : ./global_results_cicids.csv\n"
     ]
    }
   ],
   "source": [
    "summary_df = load_all_global_summaries(\"./results_cicids\")\n",
    "\n",
    "if summary_df is not None:\n",
    "    print(summary_df.head())\n",
    "\n",
    "    summary_path = \"./global_results_cicids.csv\"\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(f\"üíæ Fichier de synth√®se sauvegard√© dans : {summary_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
