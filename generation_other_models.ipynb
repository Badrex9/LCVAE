{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812a9ca6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sdv.single_table import TVAESynthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.single_table import CopulaGANSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a959de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (2827876, 79)\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Load and prepare the data\n",
    "# ==========================\n",
    "\n",
    "# Load the cleaned NSL-KDD dataset (with 5 classes grouped: normal, dos, r2l, u2r, probe)\n",
    "data = pd.read_csv(\"./dataset/cicids2017_clean_all_labels.csv\")\n",
    "print(\"Original shape:\", data.shape)\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = data.drop(columns=[\"target\"])\n",
    "y = data[\"target\"]\n",
    "\n",
    "# Split the dataset into training and testing sets (70/30), preserving class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Load the pre-trained transformer (e.g. TypedColumnTransformer)\n",
    "# This transformer handles encoding of categorical features and scaling of numerical ones\n",
    "tdt = joblib.load(\"./typed_cicids2017_all_features.pkl\")\n",
    "\n",
    "# Apply the transformation only to the training set (to avoid data leakage)\n",
    "X_train_encoded = tdt.transform(X_train)\n",
    "\n",
    "# Encode labels into integers (e.g. \"normal\" → 0, \"dos\" → 1, etc.)\n",
    "encoder_label = LabelEncoder()\n",
    "y_train_encoded = encoder_label.fit_transform(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fee0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 1:1 synthetic dataset generated: 103961 rows (from training set)\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Custom 1:1 SMOTE Generation\n",
    "# ==========================\n",
    "def custom_smote_pointwise(X_cls: np.ndarray, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    X_unique = np.unique(X_cls, axis=0)\n",
    "    if len(X_unique) < 2:\n",
    "        raise ValueError(\"Not enough unique points to interpolate.\")\n",
    "    \n",
    "    n = len(X_cls)\n",
    "    synth = []\n",
    "\n",
    "    for i in range(n):\n",
    "        x_i = X_cls[i]\n",
    "        while True:\n",
    "            x_j = X_unique[rng.integers(0, len(X_unique))]\n",
    "            if not np.allclose(x_i, x_j):\n",
    "                break\n",
    "        lam = rng.uniform(0, 1)\n",
    "        x_new = x_i + lam * (x_j - x_i)\n",
    "        synth.append(x_new)\n",
    "\n",
    "    return np.array(synth)\n",
    "\n",
    "# Synthetic generation class by class\n",
    "X_synth_list = []\n",
    "y_synth_list = []\n",
    "\n",
    "for label in np.unique(y_train_encoded):\n",
    "    X_cls = X_train_encoded[y_train_encoded == label]\n",
    "    try:\n",
    "        X_synth = custom_smote_pointwise(X_cls, random_state=label)\n",
    "    except ValueError as e:\n",
    "        print(f\"[!] Class {label} skipped: {e}\")\n",
    "        continue\n",
    "\n",
    "    X_synth_list.append(X_synth)\n",
    "    y_synth_list.append([label] * len(X_synth))\n",
    "\n",
    "# Merge\n",
    "X_synth_encoded = np.vstack(X_synth_list)\n",
    "y_synth_encoded = np.hstack(y_synth_list)\n",
    "\n",
    "# ==========================\n",
    "# Back to original space\n",
    "# ==========================\n",
    "X_synth_original = tdt.inverse_transform(X_synth_encoded)\n",
    "df_synth = pd.DataFrame(X_synth_original)\n",
    "df_synth[\"target\"] = encoder_label.inverse_transform(y_synth_encoded)\n",
    "\n",
    "# Save\n",
    "df_synth.to_csv(\"./generations_cicids/synthetic_df_smote.csv\", index=False)\n",
    "print(f\"✅ 1:1 synthetic dataset generated: {df_synth.shape[0]} rows (from training set)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdcf2708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class: 100%|██████████| 6/6 [00:34<00:00,  5.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 1:1 synthetic ADASYN dataset generated: 103955 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================\n",
    "# ADASYN generation class by class (1:1)\n",
    "# ==========================\n",
    "X_synth_list = []\n",
    "y_synth_list = []\n",
    "\n",
    "for label in tqdm(np.unique(y_train_encoded), desc=\"Class\"):\n",
    "    X_cls = X_train_encoded[y_train_encoded == label]\n",
    "    n_real = len(X_cls)\n",
    "\n",
    "    if n_real < 2:\n",
    "        print(f\"[!] Class {label} skipped (fewer than 2 points)\")\n",
    "        continue\n",
    "\n",
    "    # Create artificial dataset: class 1 (real), class 0 (fake)\n",
    "    X_cls_df = pd.DataFrame(X_cls)\n",
    "    X_fake = pd.concat([X_cls_df, X_cls_df.sample(2, random_state=42)], ignore_index=True)\n",
    "    y_fake = np.array([1] * n_real + [0, 0])\n",
    "\n",
    "    # ADASYN: generates ~n_real synthetic points for class \"1\"\n",
    "    adasyn = ADASYN(sampling_strategy={1: 2 * n_real}, random_state=label, n_neighbors=5)\n",
    "    X_res, y_res = adasyn.fit_resample(X_fake, y_fake)\n",
    "\n",
    "    # Retrieve only synthetic points\n",
    "    X_synth = X_res[len(X_fake):]\n",
    "    X_synth = X_synth[:n_real]  # limit to n_real points\n",
    "    y_synth = [label] * len(X_synth)\n",
    "\n",
    "    X_synth_list.append(pd.DataFrame(X_synth))\n",
    "    y_synth_list.append(pd.Series(y_synth))\n",
    "\n",
    "# Merge\n",
    "X_synth_encoded = pd.concat(X_synth_list).to_numpy()\n",
    "y_synth_encoded = pd.concat(y_synth_list).to_numpy()\n",
    "\n",
    "# ==========================\n",
    "# Back to original space\n",
    "# ==========================\n",
    "X_synth_original = tdt.inverse_transform(X_synth_encoded)\n",
    "df_synth = pd.DataFrame(X_synth_original)\n",
    "df_synth[\"target\"] = encoder_label.inverse_transform(y_synth_encoded)\n",
    "\n",
    "# Save\n",
    "df_synth.to_csv(\"./generations/synthetic_df_adasyn.csv\", index=False)\n",
    "print(f\"\\n✅ 1:1 synthetic ADASYN dataset generated: {df_synth.shape[0]} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce958c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TVAE\n",
    "\n",
    "# Automatically detect data types\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(data)\n",
    "\n",
    "# Train TVAE on the full dataset\n",
    "tvae = TVAESynthesizer(metadata)\n",
    "tvae.fit(data)\n",
    "\n",
    "# Generate 70% of the original dataset\n",
    "n_samples = int(len(data) * 0.7)\n",
    "synthetic_data = tvae.sample(n_samples)\n",
    "\n",
    "# Save\n",
    "synthetic_data.to_csv(\"./generations/synthetic_df_tvae.csv\", index=False)\n",
    "print(f\"✅ Synthetic data generated: {synthetic_data.shape[0]} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# CTGAN\n",
    "# ==========================\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(data)\n",
    "\n",
    "# ==========================\n",
    "# Train CTGAN on the full dataset\n",
    "# ==========================\n",
    "ctgan = CTGANSynthesizer(metadata)\n",
    "ctgan.fit(data)\n",
    "\n",
    "# ==========================\n",
    "# Generate 70% synthetic data\n",
    "# ==========================\n",
    "n_samples = int(len(data) * 0.7)\n",
    "synthetic_data = ctgan.sample(n_samples)\n",
    "\n",
    "# ==========================\n",
    "# Save\n",
    "# ==========================\n",
    "df_ctgan = synthetic_data.copy()\n",
    "df_ctgan.to_csv(\"./generations/synthetic_df_ctgan.csv\", index=False)\n",
    "\n",
    "print(f\"✅ CTGAN synthetic data generated: {df_ctgan.shape[0]} rows\")\n",
    "df_ctgan.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f755a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# CopulaGAN\n",
    "# ==========================\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(data)\n",
    "\n",
    "# ==========================\n",
    "# Train CopulaGAN\n",
    "# ==========================\n",
    "copulagan = CopulaGANSynthesizer(metadata)\n",
    "copulagan.fit(data)\n",
    "\n",
    "# ==========================\n",
    "# Generate 70% of the dataset\n",
    "# ==========================\n",
    "n_samples = int(len(data) * 0.7)\n",
    "data_copula_gan = copulagan.sample(n_samples)\n",
    "\n",
    "# ==========================\n",
    "# Save\n",
    "# ==========================\n",
    "data_copula_gan.to_csv(\"./generations/synthetic_df_copulagan.csv\", index=False)\n",
    "print(f\"✅ CopulaGAN synthetic data generated: {data_copula_gan.shape[0]} rows\")\n",
    "data_copula_gan.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
